---
layout: default
title: Rishi Bommasani
---
<div class="blurb">
<h1> Rishi Bommasani</h1>
<p> I am the Society Lead at the Stanford <a href="https://crfm.stanford.edu/">Center for Research on Foundation Models (CRFM)</a>.
<br> I am finishing my PhD at Stanford Computer Science, advised by <a href="https://cs.stanford.edu/~pliang/">Percy Liang</a> and <a href="https://web.stanford.edu/~jurafsky/">Dan Jurafsky</a>.
<br> Funding: <a href="https://vpge.stanford.edu/fellowships-funding/gerald-j-lieberman-fellowship">Lieberman Fellowship (active)</a>, <a href="https://www.nsfgrfp.org/resources/about_grfp">NSF Graduate Research Fellowship (completed)</a>. </p>

<p> Prior to Stanford, I began research at Cornell (BA Math, BA CS, MS CS) under <a href="http://www.cs.cornell.edu/home/cardie/">Claire Cardie</a>.
<br> I am honored to have worked with the <a href="https://news.psu.edu/story/660710/2021/06/07/college-engineering-mourns-loss-faculty-member-arzoo-katiyar">late</a> <a href="https://sites.google.com/site/arzook99/home">Professor Arzoo Katiyar</a>. 
<br> Cornell CS holds a special place in my heart: the department wrote this about my journey. <a href="https://www.cs.cornell.edu/information/news/newsitem11229/profile-cornell-cs-ba-and-ms-graduate-rishi-bommasani"> [Profile] </a> <a href="https://www.cs.cornell.edu/information/news/newsitem11227/lillian-lee-and-tacl-cs-graduate-stanford-new-platform-data-privacy"> [Profile 2] </a> </p>

<p> I research the societal impact of AI, especially <a href="https://arxiv.org/abs/2108.07258">foundation models</a>, to advance <a href="https://understanding-ai-safety.org/">evidence-based AI policy</a>.
<br> My research has been featured in <a href="https://www.theatlantic.com/technology/archive/2023/10/ai-technology-secrecy-transparency-index/675699/">The Atlantic</a>, <a href="https://www.axios.com/2023/06/22/ai-models-eu-law-study">Axios</a>, <a href="https://www.bloomberg.com/news/newsletters/2023-10-19/klobuchar-says-ai-regulation-still-possible-before-end-of-year?srnd=premium-europe&amp;embedded-checkout=true&amp;sref=XydSzIeb">Bloomberg</a>, <a href="https://www.euractiv.com/section/artificial-intelligence/news/ai-act-meps-close-ranks-in-asking-for-tighter-rules-for-powerful-ai-models/">Euractiv</a>, <a href="https://www.fastcompany.com/90968623/why-everyone-seems-to-disagree-on-how-to-define-artificial-general-intelligence">Fast Company</a>, <a href="https://www.ft.com/content/c325fcdd-ab29-4cd3-9a74-6cc52b28ff5f">Financial Times</a>, <a href="https://fortune.com/2023/10/24/major-ai-models-flunk-transparency-test-stanford-institute-for-human-centered-ai-report/">Fortune</a>, <a href="https://www.theinformation.com/articles/openai-is-human-after-all-sharing-is-caring-researchers-tell-model-developers?rc=lqnqnq">The Information</a>, <a href="https://www.technologyreview.com/2023/10/24/1082247/this-new-tool-could-give-artists-an-edge-over-ai/">MIT Technology Review</a>, <a href="https://www.nature.com/articles/d41586-024-00497-8">Nature</a>, <a href="https://www.nytimes.com/2023/10/18/technology/how-ai-works-stanford.html">The New York Times</a>, <a href="https://www.politico.eu/newsletter/digital-bridge/the-case-for-digital-transparency/">Politico</a>, <a href="https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/">Quanta</a>, <a href="https://www.rappler.com/technology/stanford-ai-foundation-model-transparency-index-2023-low-scores/">Rappler</a>, <a href="https://www.reuters.com/technology/stanford-researchers-issue-ai-transparency-report-urge-tech-companies-reveal-2023-10-18/">Reuters</a>, <a href="https://techpolicy.press/looking-beyond-the-black-box-transparency-and-foundation-models/">Tech Policy Press</a>, <a href="https://venturebeat.com/ai/stanford-debuts-first-ai-benchmark-to-help-understand-llms/">VentureBeat</a>, <a href="https://www.theverge.com/2023/10/18/23922973/stanford-ai-foundation-model-transparency-index">The Verge</a>, <a href="https://www.vox.com/future-perfect/23674696/chatgpt-ai-creativity-originality-homogenization">Vox</a>, <a href="https://www.wsj.com/articles/ai-talks-leave-little-tech-out-homeland-security-adversaries-open-source-board-46e3232d">The Wall Street Journal</a> and <a href="https://www.washingtonpost.com/technology/2024/03/05/ai-research-letter-openai-meta-midjourney/">The Washington Post</a>.
</p>

<div class="blurb">
<h1> Updates</h1>
<ul>
<li><b>December 2024</b> The Reality of AI and Biorisk. <a href="https://arxiv.org/abs/2412.01946">[Paper]</a> </li>
<li><b>December 2024</b> Effective Mitigations for Systemic Risks from General-Purpose AI. <a href="https://arxiv.org/abs/2412.02145">[Paper]</a> </li>
<li><b>October 2024</b> Governance of Open Foundation Models (Published in Science). <a href="https://www.science.org/doi/10.1126/science.adp1848">[Paper]</a> </li>
<li><b>October 2024</b> Language model developers should report train-test overlap. <a href="https://arxiv.org/abs/2410.08385">[Paper]</a> </li>
<li><b>September 2024</b> Science and Evidence-based AI Policy. <a href="https://understanding-ai-safety.org/">[Paper]</a> </li>
<li><b>June 2024</b> The Responsible Foundation Model Development Cheatsheet (Outstanding Survey Paper). <a href="https://fmcheatsheet.org/">[Website]</a> <a href="https://arxiv.org/abs/2406.16746">[Paper]</a> </li>
<li><b>May 2024</b> International Scientific Report on the Safety of Advanced AI. <a href="https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai">[Website]</a> <a href="https://arxiv.org/abs/2412.05282">[Paper]</a> </li>
<li><b>May 2024</b> Foundation Model Transparency Index v1.1: 6 months later. <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://crfm.stanford.edu/fmti/paper.pdf">[Paper]</a> </li>
<li><b>March 2024</b> A Safe Harbor for AI Evaluation and Red Teaming (Oral). <a href="https://sites.mit.edu/ai-safe-harbor/">[Website]</a> <a href="https://arxiv.org/abs/2403.04893">[Paper]</a> <a href="https://knightcolumbia.org/blog/a-safe-harbor-for-ai-evaluation-and-red-teaming">[Blog]</a> </li>
<li><b>February 2024</b> On the Societal Impact of Open Foundation Models (Oral). <a href="https://crfm.stanford.edu/open-fms/">[Website]</a> <a href="https://arxiv.org/pdf/2403.07918.pdf">[Paper]</a> <a href="https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf">[Policy Brief]</a> </li>
<li><b>February 2024</b> Foundation Model Transparency Reports (Oral). <a href="https://arxiv.org/abs/2402.16268">[Paper]</a> </li>
<li><b>October 2023</b> The Foundation Model Transparency Index. <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://arxiv.org/abs/2310.12941">[Paper]</a> <a href="https://www.aisnakeoil.com/p/how-transparent-are-foundation-model">[Blog]</a> </li>
<li><b>November 2022</b> Holistic Evaluation of Language Models (Best Paper). <a href="https://crfm.stanford.edu/helm">[Website]</a> <a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://crfm.stanford.edu/2022/11/17/helm.html">[Blog]</a></li> 
<li><b>August 2021</b> On the Opportunties and Risks of Foundation Models. <a href="https://crfm.stanford.edu/report.html">[Paper]</a> <a href="https://youtu.be/ZshcPdavsdU">[Explainer Video]</a> </li>	
</ul>
