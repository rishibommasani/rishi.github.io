---
layout: default
title: Rishi Bommasani
---
<div class="blurb">
<h1> Rishi Bommasani</h1>
<p> I am the Society Lead at the Stanford <a href="https://crfm.stanford.edu/">Center for Research on Foundation Models (CRFM)</a>.
<br> I am completing my PhD at Stanford Computer Science, advised by <a href="https://cs.stanford.edu/~pliang/">Percy Liang</a> and <a href="https://web.stanford.edu/~jurafsky/">Dan Jurafsky</a>.
<br> Affiliations: <a href="https://nlp.stanford.edu/">Stanford NLP</a>, <a href="https://ai.stanford.edu/">Stanford AI</a>, and <a href="https://hai.stanford.edu/">Stanford HAI</a>. Funding: <a href="https://www.nsfgrfp.org/resources/about_grfp">NSF GRFP</a>. </p>

<p> Prior to Stanford, I began research at Cornell (BA Math, BA CS, MS CS) under <a href="http://www.cs.cornell.edu/home/cardie/">Claire Cardie</a>.
<br> I am deeply honored to have learned from the <a href="https://news.psu.edu/story/660710/2021/06/07/college-engineering-mourns-loss-faculty-member-arzoo-katiyar">late</a> <a href="https://sites.google.com/site/arzook99/home">Professor Arzoo Katiyar</a>, who I profoundly miss. 
<br> Cornell CS holds a special place in my heart: the department wrote this about my journey. <a href="https://www.cs.cornell.edu/information/news/newsitem11229/profile-cornell-cs-ba-and-ms-graduate-rishi-bommasani"> [Profile] </a> <a href="https://www.cs.cornell.edu/information/news/newsitem11227/lillian-lee-and-tacl-cs-graduate-stanford-new-platform-data-privacy"> [Profile 2] </a> </p>

<p> I research the societal impact of AI, especially <a href="https://arxiv.org/abs/2108.07258">foundation models</a>.
<ul>
<li><b>Transparency</b> (establish basic facts). <a href="https://arxiv.org/abs/2211.09110">holistic evaluation</a>, <a href="https://arxiv.org/abs/2303.15772">supply chain monitoring</a>, <a href="https://arxiv.org/abs/2307.05862">ecosystem-level analysis</a>, <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649.pdf">data evaluation</a>
<li><b>Concepts</b> (introduce new ideas). <a href="https://arxiv.org/abs/2211.13972">homogeneous outcomes</a>, <a href="https://arxiv.org/abs/2212.11672">trustworthy evaluation</a>, <a href="https://arxiv.org/abs/2206.07682">emergent capabilities</a>, <a href="https://arxiv.org/abs/2206.03216">data governance</a>
<li><b>Progress</b> (improve societal outcomes). <a href="https://crfm.stanford.edu/fmti">standards</a>, <a href="https://crfm.stanford.edu/2022/05/17/community-norms.html">norms</a>, <a href="https://hai.stanford.edu/responses-ntias-request-comment-ai-accountability-policy">policy recommendations</a>, <a href="https://crfm.stanford.edu/2023/06/15/eu-ai-act.html">regulatory compliance</a>, <a href="https://crfm.stanford.edu/2023/11/18/tiers.html">proportional regulation</a>, <a href="https://arxiv.org/abs/2212.11670">power</a>
</ul>

<p>
My research has been featured in <a href="https://www.theatlantic.com/technology/archive/2023/10/ai-technology-secrecy-transparency-index/675699/">The Atlantic</a>, <a href="https://www.axios.com/2023/06/22/ai-models-eu-law-study">Axios</a>, <a href="https://www.bloomberg.com/news/newsletters/2023-10-19/klobuchar-says-ai-regulation-still-possible-before-end-of-year?srnd=premium-europe&amp;embedded-checkout=true&amp;sref=XydSzIeb">Bloomberg</a>, <a href="https://www.euractiv.com/section/artificial-intelligence/news/ai-act-meps-close-ranks-in-asking-for-tighter-rules-for-powerful-ai-models/">Euractiv</a>, <a href="https://www.fastcompany.com/90968623/why-everyone-seems-to-disagree-on-how-to-define-artificial-general-intelligence">Fast Company</a>, <a href="https://www.ft.com/content/c325fcdd-ab29-4cd3-9a74-6cc52b28ff5f">Financial Times</a>, <a href="https://fortune.com/2023/10/24/major-ai-models-flunk-transparency-test-stanford-institute-for-human-centered-ai-report/">Fortune</a>, <a href="https://www.theinformation.com/articles/openai-is-human-after-all-sharing-is-caring-researchers-tell-model-developers?rc=lqnqnq">The Information</a>, <a href="https://www.technologyreview.com/2023/10/24/1082247/this-new-tool-could-give-artists-an-edge-over-ai/">MIT Technology Review</a>, <a href="https://www.nytimes.com/2023/10/18/technology/how-ai-works-stanford.html">The New York Times</a>, <a href="https://www.politico.eu/newsletter/digital-bridge/the-case-for-digital-transparency/">Politico</a>, <a href="https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/">Quanta</a>, <a href="https://www.rappler.com/technology/stanford-ai-foundation-model-transparency-index-2023-low-scores/">Rappler</a>, <a href="https://www.reuters.com/technology/stanford-researchers-issue-ai-transparency-report-urge-tech-companies-reveal-2023-10-18/">Reuters</a>, <a href="https://techpolicy.press/looking-beyond-the-black-box-transparency-and-foundation-models/">Tech Policy Press</a>, <a href="https://venturebeat.com/ai/stanford-debuts-first-ai-benchmark-to-help-understand-llms/">VentureBeat</a>, <a href="https://www.theverge.com/2023/10/18/23922973/stanford-ai-foundation-model-transparency-index">The Verge</a> and <a href="https://www.vox.com/future-perfect/23674696/chatgpt-ai-creativity-originality-homogenization">Vox</a>.
</p>

<div class="blurb">
<h1> Updates</h1>
<ul>
<li><b>December 2023</b> Two papers at NeurIPS on measuring systemic failures across ML APIs and inference time for LM APIs. <a href="https://arxiv.org/abs/2307.05862">[Ecosystem-level Analysis]</a> <a href="https://arxiv.org/abs/2305.02440">[Efficiency Metrics]</a> </li>
<li><b>November 2023</b> Policy brief on how to design tiers for proportional regulation of foundation models. <a href="https://crfm.stanford.edu/2023/11/18/tiers.html">[Policy Brief]</a> </li>
<li><b>November 2023</b> Several analyses of the Biden Executive Order on AI. <a href="https://hai.stanford.edu/news/decoding-white-house-ai-executive-orders-achievements">[Full Analysis]</a> <a href="https://hai.stanford.edu/news/numbers-tracking-ai-executive-order">[Key Statistics]</a> <a href="https://www.aisnakeoil.com/p/what-the-executive-order-means-for">[Open Models]</a> <a href="https://docs.google.com/spreadsheets/d/1xOL4hkQ2pLR-IAs3awIiXjPLmhIeXyE5-giJ5nT-h1M/edit">[Implementation Tracker]</a> </li>
<li><b>October 2023</b> Percy, Surya and I hosted Stanford HAI's Fall Conference on New Horizons for Generative AI across Science, Creativity and Society. <a href="https://hai.stanford.edu/events/new-horizons-generative-ai-science-creativity-and-society">[Website]</a> </li>
<li><b>October 2023</b> We launched the Foundation Model Transparency Index. <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://arxiv.org/abs/2310.12941">[Paper]</a> <a href="https://www.aisnakeoil.com/p/how-transparent-are-foundation-model">[Blog]</a> </li>
<li><b>September 2023</b> Sayash, Percy, Arvind and I hosted the Princeton-Stanford Workshop on Responsible and Open Foundation Models. <a href="https://sites.google.com/view/open-foundation-models">[Website]</a> </li>
<li><b>June 2023</b> Stanford-Princeton response to the NTIA Request for Comment on AI Accountability. <a href="https://hai.stanford.edu/responses-ntias-request-comment-ai-accountability-policy">[Policy Brief]</a> </li>
<li><b>November 2022</b> We released the Holistic Evaluation of Language Models. <a href="https://crfm.stanford.edu/helm">[Website]</a> <a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://crfm.stanford.edu/2022/11/17/helm.html">[Blog]</a></li> 
<li><b>August 2021</b> We released <a href="https://arxiv.org/abs/2108.07258">On the Opportunties and Risks of Foundation Models</a>! <a href="https://crfm.stanford.edu/report.html">[Paper]</a> <a href="https://youtu.be/ZshcPdavsdU">[Explainer Video]</a> </li>	
<li><b>August 2021</b> We inaugurated the <a href="https://crfm.stanford.edu/"> Center for Research on Foundation Models (CRFM)</a> as an interdisciplinary initiative spanning 10+ departments! <a href="https://hai.stanford.edu/news/introducing-center-research-foundation-models-crfm">[Announcement Story]</a> </li>	  
</ul>
