---
layout: default
title: Papers
---
	<h1>Signature work</h1>
	<ul>
	<li><b> Holistic Evaluation of Language Models </b>
		<br> Percy Liang*, Rishi Bommasani*, Tony Lee*, <a href="https://arxiv.org/pdf/2211.09110.pdf#author-contributions">full list of authors</a>
		<br> <a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://crfm.stanford.edu/helm/">[Website]</a> <a href="https://crfm.stanford.edu/2022/11/17/helm.html">[Blog]</a> <a href="https://github.com/stanford-crfm/helm">[Code]</a>
		<br> <a href="https://venturebeat.com/ai/stanford-debuts-first-ai-benchmark-to-help-understand-llms/">[VentureBeat]</a> <a href="https://gradientflow.com/holistic-evaluation-of-language-models/">[Gradient Flow]</a> <a href="https://www.technologyreview.com/2022/11/22/1063618/trust-large-language-models-at-your-own-peril/">[MIT Technology Review]</a> </li>	
	</li>	
	<li><b> On the Opportunities and Risks of Foundation Models </b>
		<br> Rishi Bommasani*, <a href="https://crfm.stanford.edu/report.html">full list of authors</a>, Percy Liang*
		<br> <a href="https://crfm.stanford.edu/report.html">[Paper]</a> <a href="https://www.youtube.com/watch?v=ZshcPdavsdU">[Explainer Video]</a> <a href="https://crfm.stanford.edu/2021/10/18/commentaries.html">[Commentaries]</a>
		<br> <a href="https://www.discovermagazine.com/technology/the-coming-identity-crisis-for-ai">[Discover Magazine]</a> <a href="https://venturebeat.com/2021/08/18/foundation-models-risk-exacerbating-mls-ethical-challenges/">[VentureBeat]</a> <a href="https://www.fastcompany.com/90666920/ai-bias-stanford-percy-liang-fei-fei-li">[Fast Company]</a> <a href="https://www.theregister.com/2021/08/23/percy_liang_qa/">[The Register]</a> <a href="https://www.axios.com/foundation-ai-models-stanford-5562dc3f-99c1-4127-bc6a-6e4e6ecc982a.html">[Axios]</a> </li>	
	</li>	  

	<li><b> Generalized Optimal Linear Orders </b>
		<br> Rishi Bommasani
		<br> Committee: Claire Cardie (Chair), Robert Kleinberg
		<br> <b> M.S. Thesis, Cornell University </b>
		<br> <a href="https://ecommons.cornell.edu/handle/1813/103195">[Thesis]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/Thesis-Defense-2020.pdf">[Slides]</a> 
        </li>	
	</ul>

	<h1>Full list of writings</h1>
	<ul>
	<li><b> Ecosystem Graphs: The Social Footprint of Foundation Models </b>
		<br> Rishi Bommasani, Dilara Soylu, Thomas I. Liao, Kathleen A. Creel, Percy Liang
		<br> <a href="https://arxiv.org/abs/2303.15772">[Paper]</a> <a href="https://crfm.stanford.edu/ecosystem-graphs/">[Website]</a> <a href="https://crfm.stanford.edu/2023/03/29/ecosystem-graphs.html">[Blog]</a> <a href="https://github.com/stanford-crfm/ecosystem-graphs">[Code]</a>	
		<br> <a href="https://hai.stanford.edu/news/ecosystem-graphs-social-footprint-foundation-models">[HAI]</a>
	</li>	

	<li><b> AI Spring? Four Takeaways from Major Releases in Foundation Models </b>
		<br> Rishi Bommasani
		<br> <a href="https://hai.stanford.edu/news/ai-spring-four-takeaways-major-releases-foundation-models">[HAI]</a>
	</li>

	<li><b> Improving Transparency in AI Language Models: A Holistic Evaluation </b>
		<br> Rishi Bommasani, Daniel Zhang, Tony Lee, Percy Liang
		<br> <a href="https://hai.stanford.edu/sites/default/files/2023-02/HAI%20Policy%20%26%20Society%20Issue%20Brief%20-%20Improving%20Transparency%20in%20AI%20Language%20Models.pdf">[Policy Brief]</a>
	</li>	
		
	<li><b> Evaluation for Change </b>
		<br> Rishi Bommasani
		<br> <a href="https://arxiv.org/abs/2212.11670">[Paper]</a>
	</li>	
		
	<li><b> Trustworthy Social Bias Measurement </b>
		<br> Rishi Bommasani, Percy Liang
		<br> <a href="https://arxiv.org/abs/2212.11672">[Paper]</a> <a href="https://github.com/rishibommasani/BiasMeasures">[Code]</a>
	</li>	

	<li><b> Evaluating Human-Language Model Interaction </b>
		<br> Mina Lee, <a href="https://arxiv.org/abs/2212.09746">full list of authors</a>, Rishi Bommasani, Michael Bernstein, Percy Liang
		<br> <a href="https://arxiv.org/abs/2212.09746">[Paper]</a> <a href="https://github.com/minggg/halie">[Code]</a>
	</li>	
		
	<li><b> Holistic Evaluation of Language Models </b>
		<br> Percy Liang*, Rishi Bommasani*, Tony Lee*, <a href="https://arxiv.org/pdf/2211.09110.pdf#author-contributions">full list of authors</a>
		<br> <a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://crfm.stanford.edu/helm/">[Website]</a> <a href="https://crfm.stanford.edu/2022/11/17/helm.html">[Blog]</a> <a href="https://github.com/stanford-crfm/helm">[Code]</a>
		<br> <a href="https://venturebeat.com/ai/stanford-debuts-first-ai-benchmark-to-help-understand-llms/">[VentureBeat]</a> <a href="https://gradientflow.com/holistic-evaluation-of-language-models/">[Gradient Flow]</a> <a href="https://www.technologyreview.com/2022/11/22/1063618/trust-large-language-models-at-your-own-peril/">[MIT Technology Review]</a> </li>	
	</li>	

	<li><b> Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization? </b>
		<br> Rishi Bommasani, Kathleen A. Creel, Ananya Kumar, Dan Jurafsky, Percy Liang
		<br> <b> <a href="https://nips.cc/Conferences/2022">NeurIPS 2022</a> </b>
		<br> <a href="https://arxiv.org/abs/2211.13972">[Paper]</a> <a href="https://github.com/rishibommasani/HomogenizationNeurIPS2022">[Code]</a> </li>
	</li>
		
	<li><b> Emergent Abilities of Large Language Models </b>
		<br> Jason Wei, Yi Tay, Rishi Bommasani, <a href="https://arxiv.org/abs/2206.07682">full list of authors</a>, Percy Liang, Jeff Dean, William Fedus
		<br> <b> <a href="https://openreview.net/forum?id=yzkSU5zdwD">TMLR 2022</a> </b>
		<br> <a href="https://arxiv.org/abs/2206.07682">[Paper]</a> <a href="https://hai.stanford.edu/news/examining-emergent-abilities-large-language-models">[Blog]</a> <a href="https://venturebeat.com/2022/06/28/4-ai-research-trends-everyone-is-or-will-be-talking-about/">[VentureBeat]</a> </li>
	</li>
		
	<li><b> Data Governance in the Age of Large-Scale Data-Driven Language Technology </b>
		<br> Yacine Jernite, Huu Nguyen, <a href="https://facctconference.org/static/pdfs_2022/facct22-171">full list of authors</a>, Rishi Bommasani, Margaret Mitchell
		<br> <b> <a href="https://facctconference.org/2022/s">FAccT 2022</a> </b>
		<br> <a href="https://dl.acm.org/doi/abs/10.1145/3531146.3534637">[Paper]</a> </li>
	</li>
		
	<li><b> The Time Is Now to Develop Community Norms for the Release of Foundation Models </b>
		<br> Percy Liang, Rishi Bommasani, Kathleen A. Creel, Rob Reich 	  
		<br> <a href="https://crfm.stanford.edu/2022/05/17/community-norms.html">[Blog]</a> 
		<br> <a href="https://hai.stanford.edu/news/time-now-develop-community-norms-release-foundation-models">[HAI]</a> <a href="https://www.protocol.com/enterprise/foundation-models-ai-standards-stanford?sf167186232=1">[Protocol]</a> </li>		  
	</li> 
		
	<li><b> Reflections on Foundation Models </b>
		<br> Rishi Bommasani, Percy Liang 	  
		<br> <a href="https://crfm.stanford.edu/2021/10/18/reflections.html">[Blog]</a> 
		<br> <a href="https://hai.stanford.edu/news/reflections-foundation-models">[HAI]</a> <a href="https://thegradient.pub/reflections-on-foundation-models/">[The Gradient]</a> </li>		  
	</li>          
	  
	<li><b> On the Opportunities and Risks of Foundation Models </b>
		<br> Rishi Bommasani*, <a href="https://crfm.stanford.edu/report.html">full list of authors</a>, Percy Liang*
		<br> <a href="https://arxiv.org/abs/2108.07258">[arXiv]</a> <a href="https://www.youtube.com/watch?v=ZshcPdavsdU">[Explainer Video]</a> <a href="https://crfm.stanford.edu/2021/10/18/commentaries.html">[Commentaries]</a>
		<br> <a href="https://www.discovermagazine.com/technology/the-coming-identity-crisis-for-ai">[Discover Magazine]</a> <a href="https://venturebeat.com/2021/08/18/foundation-models-risk-exacerbating-mls-ethical-challenges/">[VentureBeat]</a> <a href="https://www.fastcompany.com/90666920/ai-bias-stanford-percy-liang-fei-fei-li">[Fast Company]</a> <a href="https://www.theregister.com/2021/08/23/percy_liang_qa/">[The Register]</a> <a href="https://www.axios.com/foundation-ai-models-stanford-5562dc3f-99c1-4127-bc6a-6e4e6ecc982a.html">[Axios]</a> </li>
	</li>

	<li><b> Mistral — A Journey towards Reproducible Language Model Training </b>
		<br> Siddharth Karamcheti*, Laurel Orr*, Jason Bolton, Tianyi Zhang, Karan Goel, Avanika Narayan, Rishi Bommasani, Deepak Narayanan, Tatsunori Hashimoto, Dan Jurafsky, Christopher D. Manning, Christopher Potts, Christopher Ré, Percy Liang
		<br> <a href="https://crfm.stanford.edu/blog.html">[Blog]</a> <a href="https://github.com/stanford-crfm/mistral">[Code]</a> </li>
	</li>

	<li><b> Generalized Optimal Linear Orders </b>
		<br> Rishi Bommasani
		<br> Committee: Claire Cardie (Chair), Robert Kleinberg
		<br> <b> M.S. Thesis, Cornell University </b>
		<br> <a href="https://arxiv.org/abs/2108.10692">[arXiv]</a> 
		<br> <a href="https://ecommons.cornell.edu/handle/1813/103195">[Thesis]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/Thesis-Defense-2020.pdf">[Slides]</a> 
	</li>

	<li><b> Intrinsic Evaluation of Summarization Datasets </b>
		<br> Rishi Bommasani, Claire Cardie
		<br> <b> <a href="https://2020.emnlp.org/">EMNLP 2020</a> </b>
		<br> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649.pdf">[Paper]</a> <a href="https://slideslive.com/38938755/intrinsic-evaluation-of-summarization-datasets">[Oral]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/EMNLP-2020-Slides.pdf">[Slides]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649/">[Abstract]</a> </li>
	</li>

	<li><b> Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings </b>
		<br> Rishi Bommasani, Kelly Davis, Claire Cardie
		<br> <b> <a href="https://acl2020.org/">ACL 2020</a> </b>
		<br> <a href="https://www.aclweb.org/anthology/2020.acl-main.431.pdf">[Paper]</a> <a href="https://slideslive.com/38929398/interpreting-pretrained-contextualized-representations-via-reductions-to-static-embeddings">[Oral]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/ACL-2020-Slides.pdf">[Slides]</a> <a href="https://www.aclweb.org/anthology/2020.acl-main.431.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/2020.acl-main.431/">[Abstract]</a> </li>
	</li>

	<li><b> Towards Private Synthetic Text Generation </b>
		<br> Rishi Bommasani, Steven Wu, Xanda Schofield
		<br> <b> <a href="https://sites.google.com/view/mlwithguarantees">NeurIPS 2019 Machine Learning with Guarantees Workshop</a> </b>
		<br> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/NeurIPS2019.pdf">[Paper]</a> </li>
	</li>	  

	<li><b> Long-Distance Dependencies don’t have to be Long: Simplifying through Provably (Approximately) Optimal Permutations </b>
		<br> Rishi Bommasani
		<br> <b> <a href="https://context-composition.github.io/">NeurIPS 2019 Context and Compositionality in Biological and Artificial Neural Systems Workshop</a> </b>
		<br> <a href="https://www.aclweb.org/anthology/P19-2012">[Paper]</a> <a href="https://drive.google.com/open?id=1SXICTLInnSvoDvLqphIUFxvGF-EV_Pwv">[Poster]</a> <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-2012.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/P19-2012/">[Abstract]</a></li>
	</li>	  

	<li><b> Long-Distance Dependencies don’t have to be Long: Simplifying through Provably (Approximately) Optimal Permutations </b>
		<br> Rishi Bommasani
		<br> <b> <a href="https://sites.google.com/view/acl19studentresearchworkshop/">ACL 2019 Student Research Workshop</a> </b>
		<br> <a href="https://www.aclweb.org/anthology/P19-2012">[Paper]</a> <a href="https://drive.google.com/open?id=1SXICTLInnSvoDvLqphIUFxvGF-EV_Pwv">[Poster]</a> <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-2012.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/P19-2012/">[Abstract]</a></li>
	</li>

	<li><b> Towards Understanding Position Embeddings </b>
		<br> Rishi Bommasani, Claire Cardie
		<br> <b> <a href="https://blackboxnlp.github.io/">ACL 2019 BlackboxNLP Workshop</a> </b>
		<br> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/PositionEmbeddings.pdf">[Paper]</a> <a href="https://drive.google.com/open?id=1g258QFoLHir8QXvj68Ef9cX2NA8MCq0C">[Poster]</a></li>
	</li>

	<li><b> SPARSE: Structured Prediction using Argument-Relative Structured Encoding </b>
		<br> Rishi Bommasani, Arzoo Katiyar, Claire Cardie
		<br> <b> <a href="http://structuredprediction.github.io/SPNLP19">NAACL 2019 Structured Prediction for NLP Workshop</a> </b>
		<br> <a href="https://www.aclweb.org/anthology/W19-1503">[Paper]</a> <a href="https://drive.google.com/open?id=0BwHpOd64l_vAM1BwcVM3RE52S2h6b214d0x4bERfYjNnMHhR">[Poster]</a> <a href="https://www.aclweb.org/anthology/papers/W/W19/W19-1503.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/W19-1503/">[Abstract]</a></li>
	</li>
	</ul>
