---
layout: default
title: Papers
---
	<h1>Signature work</h1>
	<ul>
	<li><b> The Foundation Model Transparency Index </b>
		<br> Rishi Bommasani*, Kevin Klyman*, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, Percy Liang
		<br> <a href="https://arxiv.org/abs/2310.12941">[Paper]</a> <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://www.aisnakeoil.com/p/how-transparent-are-foundation-model">[Blog]</a> <a href="https://github.com/stanford-crfm/fmti">[Data]</a>
		<br> <a href="https://www.theatlantic.com/technology/archive/2023/10/ai-technology-secrecy-transparency-index/675699/">[The Atlantic]</a> <a href="https://www.ft.com/content/c325fcdd-ab29-4cd3-9a74-6cc52b28ff5f">[Financial Times]</a> <a href="https://www.theinformation.com/articles/openai-is-human-after-all-sharing-is-caring-researchers-tell-model-developers?rc=lqnqnq">[The Information]</a> <a href="https://www.nytimes.com/2023/10/18/technology/how-ai-works-stanford.html">[The New York Times]</a> <a href="https://www.politico.eu/newsletter/digital-bridge/the-case-for-digital-transparency/">[Politico]</a> <a href="https://www.reuters.com/technology/stanford-researchers-issue-ai-transparency-report-urge-tech-companies-reveal-2023-10-18/">[Reuters]</a>
	</li>	
	<li><b> Holistic Evaluation of Language Models </b>
		<br> Percy Liang*, Rishi Bommasani*, Tony Lee*, <a href="https://arxiv.org/pdf/2211.09110.pdf#author-contributions">full list of authors</a>
		<br> <a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://crfm.stanford.edu/helm/">[Website]</a> <a href="https://crfm.stanford.edu/2022/11/17/helm.html">[Blog]</a> <a href="https://github.com/stanford-crfm/helm">[Code]</a>
		<br> <a href="https://venturebeat.com/ai/stanford-debuts-first-ai-benchmark-to-help-understand-llms/">[VentureBeat]</a> <a href="https://gradientflow.com/holistic-evaluation-of-language-models/">[Gradient Flow]</a> <a href="https://www.technologyreview.com/2022/11/22/1063618/trust-large-language-models-at-your-own-peril/">[MIT Technology Review]</a> </li>	
	</li>	
	<li><b> On the Opportunities and Risks of Foundation Models </b>
		<br> Rishi Bommasani*, <a href="https://crfm.stanford.edu/report.html">full list of authors</a>, Percy Liang*
		<br> <a href="https://crfm.stanford.edu/report.html">[Paper]</a> <a href="https://www.youtube.com/watch?v=ZshcPdavsdU">[Explainer Video]</a> <a href="https://crfm.stanford.edu/2021/10/18/commentaries.html">[Commentaries]</a>
		<br> <a href="https://www.discovermagazine.com/technology/the-coming-identity-crisis-for-ai">[Discover Magazine]</a> <a href="https://venturebeat.com/2021/08/18/foundation-models-risk-exacerbating-mls-ethical-challenges/">[VentureBeat]</a> <a href="https://www.fastcompany.com/90666920/ai-bias-stanford-percy-liang-fei-fei-li">[Fast Company]</a> <a href="https://www.theregister.com/2021/08/23/percy_liang_qa/">[The Register]</a> <a href="https://www.axios.com/foundation-ai-models-stanford-5562dc3f-99c1-4127-bc6a-6e4e6ecc982a.html">[Axios]</a> </li>	
	</li>	  

<!-- 	<li><b> Generalized Optimal Linear Orders </b>
		<br> Rishi Bommasani
		<br> Committee: Claire Cardie (Chair), Robert Kleinberg
		<br> <b> M.S. Thesis, Cornell University </b>
		<br> <a href="https://ecommons.cornell.edu/handle/1813/103195">[Thesis]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/Thesis-Defense-2020.pdf">[Slides]</a> 
        </li>	 -->
	</ul>

	<h1>Full list of writings</h1>
	<ul>

	<li><b> HELM Lite: Lightweight and Broad Capabilities Evaluation </b>
		<br> Percy Liang, Yifan Mai, Josselin Somerville, Farzaan Kaiyom, Tony Lee, Rishi Bommasani
		<br> <a href="https://crfm.stanford.edu/2023/12/19/helm-lite.html">[Blog Post]</a>
	</li>
		
	<li><b> Considerations for Governing Open Foundation Models </b>
		<br> Rishi Bommasani, Sayash Kapoor, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Daniel Zhang, Marietje Schaake, Daniel E. Ho, Arvind Narayanan, Percy Liang
		<br> <a href="https://hai.stanford.edu/issue-brief-considerations-governing-open-foundation-models">[Policy Brief]</a>
	</li>

	<li><b> Towards Compromise: A Concrete Two-tier Proposal for Foundation Models in the EU AI Act </b>
		<br> Rishi Bommasani, Tatsunori Hashimoto, Daniel E. Ho, Marietje Schaake, Percy Liang
		<br> <a href="https://crfm.stanford.edu/2023/12/01/ai-act-compromise.html">[Policy Brief]</a>
	</li>
		
	<li><b> Drawing Lines: Tiers for Foundation Models </b>
		<br> Rishi Bommasani
		<br> <a href="https://crfm.stanford.edu/2023/11/18/tiers.html">[Policy Brief]</a>
	</li>

	<li><b> AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing </b>
		<br> Neel Guha*, Christie M. Lawrence*, Lindsey A. Gailmard, Kit T. Rodolfa, Faiz Surani, Rishi Bommasani, Inioluwa Deborah Raji, Mariano-Florentino Cuéllar, Colleen Honigsberg, Percy Liang, Daniel E. Ho
		<br> <b> <a href="https://www.gwlr.org/">George Washington Law Review 2024</a> </b>
		<br> <a href="https://hai.stanford.edu/sites/default/files/2023-11/AI-Regulatory-Alignment.pdf">[Paper]</a> <a href="https://hai.stanford.edu/policy-brief-ai-regulatory-alignment-problem">[Policy Brief]</a>
	</li>
		
	<li><b> By the Numbers: Tracking The AI Executive Order </b>
		<br> Caroline Meinhardt, Christie M. Lawrence, Lindsey A. Gailmard, Daniel Zhang, Rishi Bommasani, Rohini Kosoglu, Peter Henderson, Russell Wald, Daniel E. Ho
		<br> <a href="https://hai.stanford.edu/news/numbers-tracking-ai-executive-order">[Blog]</a> <a href="https://docs.google.com/spreadsheets/d/1xOL4hkQ2pLR-IAs3awIiXjPLmhIeXyE5-giJ5nT-h1M/edit">[Tracker]</a>
	</li>

	<li><b> Decoding the White House AI Executive Order’s Achievements </b>
		<br> Rishi Bommasani, Christie M. Lawrence, Lindsey A. Gailmard, Caroline Meinhardt, Daniel Zhang, Peter Henderson, Russell Wald, Daniel E. Ho
		<br> <a href="https://hai.stanford.edu/news/decoding-white-house-ai-executive-orders-achievements">[Blog]</a>
	</li>
		
	<li><b> The Foundation Model Transparency Index </b>
		<br> Rishi Bommasani*, Kevin Klyman*, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, Percy Liang
		<br> <a href="https://arxiv.org/abs/2310.12941">[Paper]</a> <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://www.aisnakeoil.com/p/how-transparent-are-foundation-model">[Blog]</a> <a href="https://github.com/stanford-crfm/fmti">[Data]</a>
		<br> <a href="https://www.theatlantic.com/technology/archive/2023/10/ai-technology-secrecy-transparency-index/675699/">[The Atlantic]</a> <a href="https://www.ft.com/content/c325fcdd-ab29-4cd3-9a74-6cc52b28ff5f">[Financial Times]</a> <a href="https://www.theinformation.com/articles/openai-is-human-after-all-sharing-is-caring-researchers-tell-model-developers?rc=lqnqnq">[The Information]</a> <a href="https://www.nytimes.com/2023/10/18/technology/how-ai-works-stanford.html">[The New York Times]</a> <a href="https://www.politico.eu/newsletter/digital-bridge/the-case-for-digital-transparency/">[Politico]</a> <a href="https://www.reuters.com/technology/stanford-researchers-issue-ai-transparency-report-urge-tech-companies-reveal-2023-10-18/">[Reuters]</a>
	</li>	

	<li><b> Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes </b>
		<br> Connor Toups*, Rishi Bommasani*, Kathleen A. Creel, Sarah Bana, Dan Jurafsky, Percy Liang
		<br> <b> <a href="https://nips.cc/Conferences/2023">NeurIPS 2023</a> </b>
		<br> <a href="https://arxiv.org/abs/2307.05862">[Paper]</a> <a href="https://github.com/rishibommasani/EcosystemLevelAnalysis">[Code]</a>	
		<br> <a href="https://hai.stanford.edu/news/when-ai-systems-systemically-fail">[HAI]</a>
	</li>	

	<li><b> Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs </b>
		<br> Deepak Narayanan, Keshav Santhanam, Peter Henderson, Rishi Bommasani, Tony Lee, Percy Liang
		<br> <b> <a href="https://nips.cc/Conferences/2023">NeurIPS 2023</a> </b>
		<br> <a href="https://arxiv.org/abs/2305.02440">[Paper]</a>	
	</li>
	
	<li><b> Do Foundation Model Providers Comply with the Draft EU AI Act? </b>
		<br> Rishi Bommasani, Kevin Klyman, Daniel Zhang, Marietje Schaake, Percy Liang
		<br> <a href="https://crfm.stanford.edu/2023/06/15/eu-ai-act.html">[Policy Brief]</a>
	</li>
		
	<li><b> Stanford-Princeton Response to the US NTIA Request for Comment on AI Accountability </b>
		<br> Rishi Bommasani, Sayash Kapoor, Daniel Zhang, Arvind Narayanan, Percy Liang
		<br> <a href="https://hai.stanford.edu/responses-ntias-request-comment-ai-accountability-policy">[Response]</a>
	</li>
		
	<li><b> Ecosystem Graphs: The Social Footprint of Foundation Models </b>
		<br> Rishi Bommasani, Dilara Soylu, Thomas I. Liao, Kathleen A. Creel, Percy Liang
		<br> <a href="https://arxiv.org/abs/2303.15772">[Paper]</a> <a href="https://crfm.stanford.edu/ecosystem-graphs/">[Website]</a> <a href="https://crfm.stanford.edu/2023/03/29/ecosystem-graphs.html">[Blog]</a> <a href="https://github.com/stanford-crfm/ecosystem-graphs">[Code]</a>	
		<br> <a href="https://hai.stanford.edu/news/ecosystem-graphs-social-footprint-foundation-models">[HAI]</a>
	</li>	

	<li><b> AI Spring? Four Takeaways from Major Releases in Foundation Models </b>
		<br> Rishi Bommasani
		<br> <a href="https://hai.stanford.edu/news/ai-spring-four-takeaways-major-releases-foundation-models">[Blog]</a>
	</li>

	<li><b> Improving Transparency in AI Language Models: A Holistic Evaluation </b>
		<br> Rishi Bommasani, Daniel Zhang, Tony Lee, Percy Liang
		<br> <a href="https://hai.stanford.edu/sites/default/files/2023-02/HAI%20Policy%20%26%20Society%20Issue%20Brief%20-%20Improving%20Transparency%20in%20AI%20Language%20Models.pdf">[Policy Brief]</a>
	</li>	
		
	<li><b> Evaluation for Change </b>
		<br> Rishi Bommasani
		<br> <b> <a href="https://2023.aclweb.org/">ACL 2023</a> </b>
		<br> <a href="https://arxiv.org/abs/2212.11670">[Paper]</a>
	</li>	
		
	<li><b> Trustworthy Social Bias Measurement </b>
		<br> Rishi Bommasani, Percy Liang
		<br> <a href="https://arxiv.org/abs/2212.11672">[Paper]</a> <a href="https://github.com/rishibommasani/BiasMeasures">[Code]</a>
	</li>	

	<li><b> Evaluating Human-Language Model Interaction </b>
		<br> Mina Lee, <a href="https://arxiv.org/abs/2212.09746">full list of authors</a>, Rishi Bommasani, Michael Bernstein, Percy Liang
		<br> <b> <a href="https://jmlr.org/tmlr/">TMLR 2023</a> </b>
		<br> <a href="https://arxiv.org/abs/2212.09746">[Paper]</a> <a href="https://github.com/minggg/halie">[Code]</a>
	</li>	
		
	<li><b> Holistic Evaluation of Language Models </b>
		<br> Percy Liang*, Rishi Bommasani*, Tony Lee*, <a href="https://arxiv.org/pdf/2211.09110.pdf#author-contributions">full list of authors</a>
		<br> <b> <a href="https://jmlr.org/tmlr/">TMLR 2023</a> </b> 
		<br> <a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://crfm.stanford.edu/helm/">[Website]</a> <a href="https://crfm.stanford.edu/2022/11/17/helm.html">[Blog]</a> <a href="https://github.com/stanford-crfm/helm">[Code]</a>
		<br> <a href="https://venturebeat.com/ai/stanford-debuts-first-ai-benchmark-to-help-understand-llms/">[VentureBeat]</a> <a href="https://gradientflow.com/holistic-evaluation-of-language-models/">[Gradient Flow]</a> <a href="https://www.technologyreview.com/2022/11/22/1063618/trust-large-language-models-at-your-own-peril/">[MIT Technology Review]</a>	
		<br> <b> Outstanding Paper </b>
	</li>	

	<li><b> Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization? </b>
		<br> Rishi Bommasani, Kathleen A. Creel, Ananya Kumar, Dan Jurafsky, Percy Liang
		<br> <b> <a href="https://nips.cc/Conferences/2022">NeurIPS 2022</a> </b>
		<br> <a href="https://arxiv.org/abs/2211.13972">[Paper]</a> <a href="https://github.com/rishibommasani/HomogenizationNeurIPS2022">[Code]</a> </li>
	</li>
		
	<li><b> Emergent Abilities of Large Language Models </b>
		<br> Jason Wei, Yi Tay, Rishi Bommasani, <a href="https://arxiv.org/abs/2206.07682">full list of authors</a>, Percy Liang, Jeff Dean, William Fedus
		<br> <b> <a href="https://openreview.net/forum?id=yzkSU5zdwD">TMLR 2022</a> </b>
		<br> <a href="https://arxiv.org/abs/2206.07682">[Paper]</a> <a href="https://hai.stanford.edu/news/examining-emergent-abilities-large-language-models">[Blog]</a> <a href="https://venturebeat.com/2022/06/28/4-ai-research-trends-everyone-is-or-will-be-talking-about/">[VentureBeat]</a>
		<br> <b> Outstanding Survey Paper </b>
	</li>
		
	<li><b> Data Governance in the Age of Large-Scale Data-Driven Language Technology </b>
		<br> Yacine Jernite, Huu Nguyen, <a href="https://facctconference.org/static/pdfs_2022/facct22-171">full list of authors</a>, Rishi Bommasani, Margaret Mitchell
		<br> <b> <a href="https://facctconference.org/2022/s">FAccT 2022</a> </b>
		<br> <a href="https://dl.acm.org/doi/abs/10.1145/3531146.3534637">[Paper]</a> </li>
	</li>
		
	<li><b> The Time Is Now to Develop Community Norms for the Release of Foundation Models </b>
		<br> Percy Liang, Rishi Bommasani, Kathleen A. Creel, Rob Reich 	  
		<br> <a href="https://crfm.stanford.edu/2022/05/17/community-norms.html">[Blog]</a> 
		<br> <a href="https://hai.stanford.edu/news/time-now-develop-community-norms-release-foundation-models">[HAI]</a> <a href="https://www.protocol.com/enterprise/foundation-models-ai-standards-stanford?sf167186232=1">[Protocol]</a> </li>		  
	</li> 
		
	<li><b> Reflections on Foundation Models </b>
		<br> Rishi Bommasani, Percy Liang 	  
		<br> <a href="https://crfm.stanford.edu/2021/10/18/reflections.html">[Blog]</a> 
		<br> <a href="https://hai.stanford.edu/news/reflections-foundation-models">[HAI]</a> <a href="https://thegradient.pub/reflections-on-foundation-models/">[The Gradient]</a> </li>		  
	</li>          
	  
	<li><b> On the Opportunities and Risks of Foundation Models </b>
		<br> Rishi Bommasani*, <a href="https://crfm.stanford.edu/report.html">full list of authors</a>, Percy Liang*
		<br> <a href="https://arxiv.org/abs/2108.07258">[arXiv]</a> <a href="https://www.youtube.com/watch?v=ZshcPdavsdU">[Explainer Video]</a> <a href="https://crfm.stanford.edu/2021/10/18/commentaries.html">[Commentaries]</a>
		<br> <a href="https://www.discovermagazine.com/technology/the-coming-identity-crisis-for-ai">[Discover Magazine]</a> <a href="https://venturebeat.com/2021/08/18/foundation-models-risk-exacerbating-mls-ethical-challenges/">[VentureBeat]</a> <a href="https://www.fastcompany.com/90666920/ai-bias-stanford-percy-liang-fei-fei-li">[Fast Company]</a> <a href="https://www.theregister.com/2021/08/23/percy_liang_qa/">[The Register]</a> <a href="https://www.axios.com/foundation-ai-models-stanford-5562dc3f-99c1-4127-bc6a-6e4e6ecc982a.html">[Axios]</a> </li>
	</li>

	<li><b> Mistral — A Journey towards Reproducible Language Model Training </b>
		<br> Siddharth Karamcheti*, Laurel Orr*, Jason Bolton, Tianyi Zhang, Karan Goel, Avanika Narayan, Rishi Bommasani, Deepak Narayanan, Tatsunori Hashimoto, Dan Jurafsky, Christopher D. Manning, Christopher Potts, Christopher Ré, Percy Liang
		<br> <a href="https://crfm.stanford.edu/blog.html">[Blog]</a> <a href="https://github.com/stanford-crfm/mistral">[Code]</a> </li>
	</li>

	<li><b> Generalized Optimal Linear Orders </b>
		<br> Rishi Bommasani
		<br> Committee: Claire Cardie (Chair), Robert Kleinberg
		<br> <b> M.S. Thesis, Cornell University </b>
		<br> <a href="https://arxiv.org/abs/2108.10692">[arXiv]</a> 
		<br> <a href="https://ecommons.cornell.edu/handle/1813/103195">[Thesis]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/Thesis-Defense-2020.pdf">[Slides]</a> 
	</li>

	<li><b> Intrinsic Evaluation of Summarization Datasets </b>
		<br> Rishi Bommasani, Claire Cardie
		<br> <b> <a href="https://2020.emnlp.org/">EMNLP 2020</a> </b>
		<br> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649.pdf">[Paper]</a> <a href="https://slideslive.com/38938755/intrinsic-evaluation-of-summarization-datasets">[Oral]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/EMNLP-2020-Slides.pdf">[Slides]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649/">[Abstract]</a> </li>
	</li>

	<li><b> Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings </b>
		<br> Rishi Bommasani, Kelly Davis, Claire Cardie
		<br> <b> <a href="https://acl2020.org/">ACL 2020</a> </b>
		<br> <a href="https://www.aclweb.org/anthology/2020.acl-main.431.pdf">[Paper]</a> <a href="https://slideslive.com/38929398/interpreting-pretrained-contextualized-representations-via-reductions-to-static-embeddings">[Oral]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/ACL-2020-Slides.pdf">[Slides]</a> <a href="https://www.aclweb.org/anthology/2020.acl-main.431.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/2020.acl-main.431/">[Abstract]</a> </li>
	</li>

	<li><b> Towards Private Synthetic Text Generation </b>
		<br> Rishi Bommasani, Steven Wu, Xanda Schofield
		<br> <b> <a href="https://sites.google.com/view/mlwithguarantees">NeurIPS 2019 Machine Learning with Guarantees Workshop</a> </b>
		<br> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/NeurIPS2019.pdf">[Paper]</a> </li>
	</li>	  

	<li><b> Long-Distance Dependencies don’t have to be Long: Simplifying through Provably (Approximately) Optimal Permutations </b>
		<br> Rishi Bommasani
		<br> <b> <a href="https://context-composition.github.io/">NeurIPS 2019 Context and Compositionality in Biological and Artificial Neural Systems Workshop</a> </b>
		<br> <a href="https://www.aclweb.org/anthology/P19-2012">[Paper]</a> <a href="https://drive.google.com/open?id=1SXICTLInnSvoDvLqphIUFxvGF-EV_Pwv">[Poster]</a> <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-2012.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/P19-2012/">[Abstract]</a></li>
	</li>	  

	<li><b> Long-Distance Dependencies don’t have to be Long: Simplifying through Provably (Approximately) Optimal Permutations </b>
		<br> Rishi Bommasani
		<br> <b> <a href="https://sites.google.com/view/acl19studentresearchworkshop/">ACL 2019 Student Research Workshop</a> </b>
		<br> <a href="https://www.aclweb.org/anthology/P19-2012">[Paper]</a> <a href="https://drive.google.com/open?id=1SXICTLInnSvoDvLqphIUFxvGF-EV_Pwv">[Poster]</a> <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-2012.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/P19-2012/">[Abstract]</a></li>
	</li>

	<li><b> Towards Understanding Position Embeddings </b>
		<br> Rishi Bommasani, Claire Cardie
		<br> <b> <a href="https://blackboxnlp.github.io/">ACL 2019 BlackboxNLP Workshop</a> </b>
		<br> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/PositionEmbeddings.pdf">[Paper]</a> <a href="https://drive.google.com/open?id=1g258QFoLHir8QXvj68Ef9cX2NA8MCq0C">[Poster]</a></li>
	</li>

	<li><b> SPARSE: Structured Prediction using Argument-Relative Structured Encoding </b>
		<br> Rishi Bommasani, Arzoo Katiyar, Claire Cardie
		<br> <b> <a href="http://structuredprediction.github.io/SPNLP19">NAACL 2019 Structured Prediction for NLP Workshop</a> </b>
		<br> <a href="https://www.aclweb.org/anthology/W19-1503">[Paper]</a> <a href="https://drive.google.com/open?id=0BwHpOd64l_vAM1BwcVM3RE52S2h6b214d0x4bERfYjNnMHhR">[Poster]</a> <a href="https://www.aclweb.org/anthology/papers/W/W19/W19-1503.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/W19-1503/">[Abstract]</a></li>
	</li>
	</ul>
