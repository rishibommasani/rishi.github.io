---
layout: default
title: Papers
---
	<h1>Signature work</h1>
	<ul>
	<li><b> On the Societal Impact of Open Foundation Models </b>
		<br> Sayash Kapoor*, Rishi Bommasani* et al.
		<br> <b> ICML 2024 (Oral, top 1.5% of papers) </b>
		<br> <a href="https://arxiv.org/abs/2403.07918">[Paper]</a> <a href="https://crfm.stanford.edu/open-fms/">[Website]</a>
	</li>	

	<li><b> The Foundation Model Transparency Index </b>
		<br> Rishi Bommasani*, Kevin Klyman* et al.
		<br> <a href="https://arxiv.org/abs/2310.12941">[Paper]</a> <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://www.aisnakeoil.com/p/how-transparent-are-foundation-model">[Blog]</a> <a href="https://github.com/stanford-crfm/fmti">[Data]</a>
	</li>	

	<li><b> Holistic Evaluation of Language Models </b>
		<br> Percy Liang*, Rishi Bommasani*, Tony Lee* et al.
		<br> <b> TMLR 2023 (Outstanding Paper) </b>
		<br> <a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://crfm.stanford.edu/helm/">[Website]</a> <a href="https://crfm.stanford.edu/2022/11/17/helm.html">[Blog]</a> <a href="https://github.com/stanford-crfm/helm">[Code]</a>
	</li>	

	<li><b> On the Opportunities and Risks of Foundation Models </b>
		<br> Rishi Bommasani*, ..., Percy Liang*
		<br> <a href="https://crfm.stanford.edu/report.html">[Paper]</a> <a href="https://www.youtube.com/watch?v=ZshcPdavsdU">[Explainer Video]</a> <a href="https://crfm.stanford.edu/2021/10/18/commentaries.html">[Commentaries]</a>
	</li>	  
	</ul>

	<h1>Full list</h1>
	<ul>

	<li><b> International Scientific Report on the Safety of Advanced AI </b>
		<br> Yoshua Bengio et al.
		<br> <b> 2024 AI Seoul Summit </b>
		<br> <a href="https://assets.publishing.service.gov.uk/media/66474eab4f29e1d07fadca3d/international_scientific_report_on_the_safety_of_advanced_ai_interim_report.pdf">[Paper]</a> <a href="https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai">[Website]</a>
	</li>	

	<li><b> The Foundation Model Transparency Index v1.1: May 2024 </b>
		<br> Rishi Bommasani*, Kevin Klyman* et al.
		<br> <a href="https://crfm.stanford.edu/fmti/paper.pdf">[Paper]</a> <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://github.com/stanford-crfm/fmti">[Data]</a>
	</li>			
		
	<li><b> On the Societal Impact of Open Foundation Models </b>
		<br> Sayash Kapoor*, Rishi Bommasani* et al.
		<br> <b> ICML 2024 (Oral, top 1.5% of papers) </b>
		<br> <a href="https://arxiv.org/abs/2403.07918">[Paper]</a> <a href="https://crfm.stanford.edu/open-fms/">[Website]</a>
	</li>	

	<li><b> A Safe Harbor for AI Evaluation and Red Teaming </b>
		<br> Shayne Longpre et al.
		<br> <b> ICML 2024 (Oral, top 1.5% of papers) </b>
		<br> <a href="https://arxiv.org/abs/2403.04893">[Paper]</a> <a href="https://sites.mit.edu/ai-safe-harbor/">[Website]</a>
	</li>	

	<li><b> Foundation Model Transparency Reports </b>
		<br> Rishi Bommasani et al.
		<br> <a href="https://arxiv.org/abs/2402.16268">[Paper]</a>
	</li>	
		
	<li><b> HELM Lite: Lightweight and Broad Capabilities Evaluation </b>
		<br> Percy Liang, Yifan Mai, Josselin Somerville, Farzaan Kaiyom, Tony Lee, Rishi Bommasani
		<br> <a href="https://crfm.stanford.edu/2023/12/19/helm-lite.html">[Blog]</a>
	</li>
		

	<li><b> AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing </b>
		<br> Neel Guha*, Christie M. Lawrence* et al.
		<br> <b> George Washington Law Review 2024 </b>
		<br> <a href="https://hai.stanford.edu/sites/default/files/2023-11/AI-Regulatory-Alignment.pdf">[Paper]</a> <a href="https://hai.stanford.edu/policy-brief-ai-regulatory-alignment-problem">[Policy Brief]</a>
	</li>
				
	<li><b> The Foundation Model Transparency Index </b>
		<br> Rishi Bommasani*, Kevin Klyman* et al.
		<br> <a href="https://arxiv.org/abs/2310.12941">[Paper]</a> <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://www.aisnakeoil.com/p/how-transparent-are-foundation-model">[Blog]</a> <a href="https://github.com/stanford-crfm/fmti">[Data]</a>
	</li>	

	<li><b> Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes </b>
		<br> Connor Toups*, Rishi Bommasani*, Kathleen A. Creel, Sarah Bana, Dan Jurafsky, Percy Liang
		<br> <b> NeurIPS 2023 </b>
		<br> <a href="https://arxiv.org/abs/2307.05862">[Paper]</a> <a href="https://github.com/rishibommasani/EcosystemLevelAnalysis">[Code]</a>	
	</li>	

	<li><b> Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs </b>
		<br> Deepak Narayanan, Keshav Santhanam, Peter Henderson, Rishi Bommasani, Tony Lee, Percy Liang
		<br> <b> NeurIPS 2023 </b>
		<br> <a href="https://arxiv.org/abs/2305.02440">[Paper]</a>	
	</li>
		
	<li><b> Ecosystem Graphs: The Social Footprint of Foundation Models </b>
		<br> Rishi Bommasani, Dilara Soylu, Thomas I. Liao, Kathleen A. Creel, Percy Liang
		<br> <a href="https://arxiv.org/abs/2303.15772">[Paper]</a> <a href="https://crfm.stanford.edu/ecosystem-graphs/">[Website]</a> <a href="https://crfm.stanford.edu/2023/03/29/ecosystem-graphs.html">[Blog]</a> <a href="https://github.com/stanford-crfm/ecosystem-graphs">[Code]</a>	
	</li>	

	<li><b> AI Spring? Four Takeaways from Major Releases in Foundation Models </b>
		<br> Rishi Bommasani
		<br> <a href="https://hai.stanford.edu/news/ai-spring-four-takeaways-major-releases-foundation-models">[Blog]</a>
	</li>
		
	<li><b> Evaluation for Change </b>
		<br> Rishi Bommasani
		<br> <b> ACL 2023 </b>
		<br> <a href="https://arxiv.org/abs/2212.11670">[Paper]</a>
	</li>	
		
	<li><b> Trustworthy Social Bias Measurement </b>
		<br> Rishi Bommasani, Percy Liang
		<br> <a href="https://arxiv.org/abs/2212.11672">[Paper]</a> <a href="https://github.com/rishibommasani/BiasMeasures">[Code]</a>
	</li>	

	<li><b> Evaluating Human-Language Model Interaction </b>
		<br> Mina Lee et al.
		<br> <b> TMLR 2023 </b>
		<br> <a href="https://arxiv.org/abs/2212.09746">[Paper]</a> <a href="https://github.com/minggg/halie">[Code]</a>
	</li>	
		
	<li><b> Holistic Evaluation of Language Models </b>
		<br> Percy Liang*, Rishi Bommasani*, Tony Lee* et al.
		<br> <b> TMLR 2023 </b> 
		<br> <a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://crfm.stanford.edu/helm/">[Website]</a> <a href="https://crfm.stanford.edu/2022/11/17/helm.html">[Blog]</a> <a href="https://github.com/stanford-crfm/helm">[Code]</a>
		<br> <b> Outstanding Paper </b>
	</li>	

	<li><b> Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization? </b>
		<br> Rishi Bommasani, Kathleen A. Creel, Ananya Kumar, Dan Jurafsky, Percy Liang
		<br> <b> NeurIPS 2022 </b>
		<br> <a href="https://arxiv.org/abs/2211.13972">[Paper]</a> <a href="https://github.com/rishibommasani/HomogenizationNeurIPS2022">[Code]</a>
	</li>
		
	<li><b> Emergent Abilities of Large Language Models </b>
		<br> Jason Wei, Yi Tay, Rishi Bommasani et al.
		<br> <b> TMLR 2022 </b>
		<br> <a href="https://arxiv.org/abs/2206.07682">[Paper]</a> <a href="https://hai.stanford.edu/news/examining-emergent-abilities-large-language-models">[Blog]</a>
		<br> <b> Outstanding Survey Paper </b>
	</li>
		
	<li><b> Data Governance in the Age of Large-Scale Data-Driven Language Technology </b>
		<br> Yacine Jernite et al.
		<br> <b> FAccT 2022 </b>
		<br> <a href="https://dl.acm.org/doi/abs/10.1145/3531146.3534637">[Paper]</a>
	</li>
		
	<li><b> The Time Is Now to Develop Community Norms for the Release of Foundation Models </b>
		<br> Percy Liang, Rishi Bommasani, Kathleen A. Creel, Rob Reich 	  
		<br> <a href="https://crfm.stanford.edu/2022/05/17/community-norms.html">[Blog]</a> <a href="https://www.protocol.com/enterprise/foundation-models-ai-standards-stanford?sf167186232=1">[Op-ed]</a>		  
	</li> 
		
	<li><b> Reflections on Foundation Models </b>
		<br> Rishi Bommasani, Percy Liang 	  
		<br> <a href="https://crfm.stanford.edu/2021/10/18/reflections.html">[Blog]</a> 
	</li>          
	  
	<li><b> On the Opportunities and Risks of Foundation Models </b>
		<br> Rishi Bommasani*, <a href="https://crfm.stanford.edu/report.html">full list of authors</a>, Percy Liang*
		<br> <a href="https://arxiv.org/abs/2108.07258">[arXiv]</a> <a href="https://www.youtube.com/watch?v=ZshcPdavsdU">[Explainer Video]</a> <a href="https://crfm.stanford.edu/2021/10/18/commentaries.html">[Commentaries]</a>
	</li>

	<li><b> Mistral â€” A Journey towards Reproducible Language Model Training </b>
		<br> Siddharth Karamcheti*, Laurel Orr* et al.
		<br> <a href="https://crfm.stanford.edu/blog.html">[Blog]</a> <a href="https://github.com/stanford-crfm/mistral">[Code]</a>
	</li>

	<li><b> Generalized Optimal Linear Orders </b>
		<br> Rishi Bommasani
		<br> Committee: Claire Cardie (Chair), Robert Kleinberg
		<br> <b> M.S. Thesis, Cornell University </b>
		<br> <a href="https://arxiv.org/abs/2108.10692">[arXiv]</a> <a href="https://ecommons.cornell.edu/handle/1813/103195">[Thesis]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/Thesis-Defense-2020.pdf">[Slides]</a> 
	</li>

	<li><b> Intrinsic Evaluation of Summarization Datasets </b>
		<br> Rishi Bommasani, Claire Cardie
		<br> <b> EMNLP 2020 </b>
		<br> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649.pdf">[Paper]</a> <a href="https://slideslive.com/38938755/intrinsic-evaluation-of-summarization-datasets">[Oral]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/EMNLP-2020-Slides.pdf">[Slides]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649/">[Abstract]</a> </li>
	</li>

	<li><b> Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings </b>
		<br> Rishi Bommasani, Kelly Davis, Claire Cardie
		<br> <b> ACL 2020 </b>
		<br> <a href="https://www.aclweb.org/anthology/2020.acl-main.431.pdf">[Paper]</a> <a href="https://slideslive.com/38929398/interpreting-pretrained-contextualized-representations-via-reductions-to-static-embeddings">[Oral]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/ACL-2020-Slides.pdf">[Slides]</a> <a href="https://www.aclweb.org/anthology/2020.acl-main.431.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/2020.acl-main.431/">[Abstract]</a> </li>
	</li>

	<li><b> Towards Private Synthetic Text Generation </b>
		<br> Rishi Bommasani, Steven Wu, Xanda Schofield
		<br> <b> NeurIPS 2019 Machine Learning with Guarantees Workshop </b>
		<br> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/NeurIPS2019.pdf">[Paper]</a> </li>
	</li>	  

	<li><b> Long-Distance Dependencies donâ€™t have to be Long: Simplifying through Provably (Approximately) Optimal Permutations </b>
		<br> Rishi Bommasani
		<br> <b> NeurIPS 2019 Context and Compositionality in Biological and Artificial Neural Systems Workshop </b>
		<br> <a href="https://www.aclweb.org/anthology/P19-2012">[Paper]</a> <a href="https://drive.google.com/open?id=1SXICTLInnSvoDvLqphIUFxvGF-EV_Pwv">[Poster]</a> <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-2012.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/P19-2012/">[Abstract]</a></li>
	</li>	  

	<li><b> Long-Distance Dependencies donâ€™t have to be Long: Simplifying through Provably (Approximately) Optimal Permutations </b>
		<br> Rishi Bommasani
		<br> <b> ACL 2019 </b>
		<br> <a href="https://www.aclweb.org/anthology/P19-2012">[Paper]</a> <a href="https://drive.google.com/open?id=1SXICTLInnSvoDvLqphIUFxvGF-EV_Pwv">[Poster]</a> <a href="https://www.aclweb.org/anthology/papers/P/P19/P19-2012.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/P19-2012/">[Abstract]</a></li>
	</li>

	<li><b> Towards Understanding Position Embeddings </b>
		<br> Rishi Bommasani, Claire Cardie
		<br> <b> ACL 2019 BlackboxNLP Workshop </b>
		<br> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/PositionEmbeddings.pdf">[Paper]</a> <a href="https://drive.google.com/open?id=1g258QFoLHir8QXvj68Ef9cX2NA8MCq0C">[Poster]</a></li>
	</li>

	<li><b> SPARSE: Structured Prediction using Argument-Relative Structured Encoding </b>
		<br> Rishi Bommasani, Arzoo Katiyar, Claire Cardie
		<br> <b> NAACL 2019 Structured Prediction for NLP Workshop </b>
		<br> <a href="https://www.aclweb.org/anthology/W19-1503">[Paper]</a> <a href="https://drive.google.com/open?id=0BwHpOd64l_vAM1BwcVM3RE52S2h6b214d0x4bERfYjNnMHhR">[Poster]</a> <a href="https://www.aclweb.org/anthology/papers/W/W19/W19-1503.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/W19-1503/">[Abstract]</a></li>
	</li>
	</ul>
